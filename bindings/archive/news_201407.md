##Clusters

Local clusters and centroids
----------------------------

*Affects:* New objects to be
created (`Cluster` --resembling `Model`--, and `Centroid`)

*Description:* The goal is to download the JSON `cluster` resource and build
a `Cluster` object that provides a method to produce the centroid that new
input data is closer to locally. Centroids are the points
that are considerd to be the centers of the clusters. They are, thus, defined
by their corresponding (field, value) sets. The distance from a new point
(input data) to a centroid is computed as a Euclidean distance where each
field has an associated scaling factor. The cluster info encloses the set
of scaling factors per field. Thus, the contribution of each component (field)
to the square of distance is:

- for numeric fields: (input_field - centroid_field)^2 * field_scale^2

- for categorical fields:
    0 if input_field == centroid_field
    field_scale^2 if input_field != centroid_field or input_field is missing

- for text fields:
   The squared distance between "vectors" of unique terms for the centroid and
   your input data. Each text field has a dict of equivalent term forms in its
   `['summary']['term_forms']` key (look for the 'fields' key in the cluster
   JSON and each field id key there has it). The key in this dict is the
   main form and the value is a list of derived forms, this kind of information
   is built in the cluster by stemming the words in the training data.
   At the same level in the JSON structure, you can find also a list
   of main words under the `['summary']['tag_cloud']`. Finally you must
   retrieve also the `['term_analysis']` options, where `case_sensitive`
   attribute is. The computation must find the distance as a scalar product
   of vectors of unique words. First you parse the words in your input data
   text field, and compare them to the tag cloud or the list of derived terms
   in term forms to build a list of the main forms corresponding to the
   words in your input data text field. This set of words is compared to each
   centroid list of main terms (as found in the centroid value) and
   cosine similarity is computed: if the term is in both of them, similarity is
   increased.


```python
    input_count = 0
    for term in centroid_terms:
        if term in terms:
            input_count += 1
    cosine_similarity = input_count / math.sqrt(
        len(terms) * len(centroid_terms))
```

    Then the contribution to the squared distance is computed as a kind of
    inverse function:

```python
    similarity_distance = scale * (1 - cosine_similarity)
    return similarity_distance ** 2
```

    There are two limit cases, when

```python
    if not terms and not centroid_terms:
        return 0
    if not terms or not centroid_terms:
        return scale ** 2
```

    Finally, there are two changes that must be considered depending on
    the `term_analysis` information. If `token_mode` is `full_terms_only`
    (see the `term_analysis` attributes) then no
    parsing is done to your input data, and if `case_sensitive` is False, the
    input data text must be lower-cased before the parsing.

You can have a look at the
[Cluster](https://github.com/bigmlcom/python/blob/next/bigml/cluster.py) and
[Centroid](https://github.com/bigmlcom/python/blob/next/bigml/centroid.py)
objects code in the
corresponding files of the python bindings and the [LocalCluster](https://github.com/bigmlcom/bigml-node/blob/master/lib/LocalCluster.js)
and [LocalCentroid](https://github.com/bigmlcom/bigml-node/blob/master/lib/LocalCentroid.js) versions in
the node.js bindings as reference.

*Test samples:*

Some tests are available in the python test suite, based on the
[data/spam.csv](data/spam.csv)
file contents.
